<!DOCTYPE html>
<html>
  <head>
    <title>BiasVar</title>
    <meta charset="utf-8">
    <link rel="stylesheet" href="https://unpkg.com/purecss@1.0.1/build/pure-min.css" integrity="sha384-oAOxQR6DkCoMliIh8yFnu25d7Eq/PHS21PClpwjOTeU2jRSq11vu66rf90/cZr47" crossorigin="anonymous">    <style>
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
      .reference{
      	font-size: 10px;
      }
      .smaller-font { font-size:14px } 
      @page {
        size: 908px 681px;
        margin: 0;
      }

      @media print {
        .remark-slide-scaler {
          width: 100% !important;
          height: 100% !important;
          transform: scale(1) !important;
          top: 0 !important;
          left: 0 !important;
        }
      }

      .figure img{
        height: 550px;
      }

      .figure-250 img{
        height: 250px;
      }

      .figure-300 img{
        height: 300px;
      }

      .figure-350 img{
        height: 350px;
      }

      .figure-w650 img{
        width: 650px;
      }
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

# Bias-Variance Tradeoff

CS534 - Machine Learning

Yubin Park, PhD

---

class: center, middle

Complex model <i>vs.</i> Simple model

Which way to go?

Bias-Variance Framework can provide 

some guidance

---

class: center, middle

Consider a system

$$ y = g(\mathbf{x}) + \epsilon $$

where `\(\text{E}[\epsilon] = 0\)` and `\(\text{Var}[\epsilon] = \sigma^2 \)`

---

## Notations

Assume that we generate a finite number of samples from this system.

`\(\text{E}[\cdot]\)`: [Expectation](https://en.wikipedia.org/wiki/Expected_value) operator. e.g. `\(\text{E}[y]\)` is the expected value of `\(y\)`.

`\(\text{Var}[\cdot]\)`: [Variance](https://en.wikipedia.org/wiki/Variance) operator.

`\(g\)`: True model

`\(f\)`: Our model trained on **finite** samples

`\(\text{E}[f]\)`: Expectation of `\(f\)`. In other words, `\(f\)` trained on **infinite** samples.

`\(\text{Var}[f]\)`: Variance of `\(f\)`. In other words, the variability of `\(f\)` due to the sampling nature. Note `\(\text{Var}[f] = \text{E}[f^2] - \text{E}[f]^2\)`

`\(\text{E}[(y-f)^2]\)`: Expected error. NOTE that this is not the same as training or test errors.

---

class: center, middle

$$\text{E}[(y-f)^2] = \text{E}[(g + \epsilon - f + \text{E}[f] - \text{E}[f])^2] $$

$$ = \text{E}[((g- \text{E}[f]) + (\text{E}[f]- f) + \epsilon)^2]$$

$$ = \text{E}[(g- \text{E}[f])^2] + \text{E}[(\text{E}[f]- f))^2] + \text{E}[\epsilon^2]$$

$$ = \text{Bias}[f]^2 + \text{Var}[f] + \sigma^2 $$

The error consists of 

Bias, Variance, and Irreducible Error. 

---

## What is Bias?

$$ \text{Bias}[f] = g- \text{E}[f] $$ 

**Bias** is the difference between the true model, `\(g\)`, and the (theoretical) best-case of our model, `\(\text{E}[f]\)`

For example, consider `\(g(x) = \exp(x)\)`, while `\(f(x) = \sum_{k=0}^K \beta_k x^k\)`. 

As finite-degree polynomial functions cannot behave like exponential functions, there will be always some gap between the true and our models. 

This theoretical gap is called "bias".

Bias is related to the expressibility (or representational capacity) of a model.

In general, simpler models have higher bias.

---

## What is Variance?

$$ \text{Var}[f] = \text{E}[(\text{E}[f]- f))^2] $$

**Variance** is the variability of our model due to the sampling nature of the data.

For example, the coefficients, `\(\beta\)`, of a linear model will vary depending on selected samples. 

The degree of such variability is called the variance.

In other words, our model has high variance if our model changes a lot with inclusion/exlusion of data samples.

NOTE that we are measuring the variability against the expectation of our model, not the true model.

In general, simpler models have less variance.

---

class: center, middle

Conceptually something like this?

.figure-350[![bias-variance](img/bv-illust.png)]

.reference[http://scott.fortmann-roe.com/docs/BiasVariance.html]

---

class: center, middle

Model Complexity and Bias-Variance

.figure-350[![bias-variance](img/bv-tb.png)]

.reference[Chapter 7 of https://web.stanford.edu/~hastie/ElemStatLearn/]

---

class: center, middle

Why Shrinkage Models can be Better?

.figure-350[![shrinkage](img/shrinkage.png)]

.reference[Chapter 7 of https://web.stanford.edu/~hastie/ElemStatLearn/]

---

class: center, middle

Overfitting vs. Underfitting

.figure-w650[![shrinkage](img/sphx_glr_plot_underfitting_overfitting_001.png)]

Underfitting (left most) = High Bias

Overfitting (right most) = High Variance

.reference[https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html]

---

class: center, middle

Assume the Irreducible Error is very small

and the size of training data is fairly large.

$$ \text{Bias}^2 = \text{E}[(g - \text{E}[f])^2]$$

$$ \approx \text{Average}[ (y - f)^2 ]$$

$$ = \text{Training Error} $$

---

class: center, middle

Assume our model is complex enough

$$ \text{i.e. }\text{E}[f] = g $$

and the Irreducible Error is ver small

and also the size of test data is fairly large.

$$ \text{Variance} = E[(f - \text{E}[f])^2]$$

$$ \approx \text{Average}[(y - f)^2]$$

$$ = \text{Test Error} $$

---

class: center, middle

How to Use the Bias-Variance

.figure-300[![ng](img/recipe.png)]

.reference[Adapted from Andrew Ng's "Applied Bias-Variance for Deep Learning Flowchart"
for building better deep learning systems, http://www.computervisionblog.com/2016/12/nuts-and-bolts-of-building-deep.html]

---

class: center, middle

## Questions?

</textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js">
    </script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML&delayStartupUntil=configured" type="text/javascript">
    </script>
    <script type="text/javascript">
      var hljs = remark.highlighter.engine;
      /*
        Language: terminal console
        Author: Josh Bode <joshbode@gmail.com>
      */
      hljs.registerLanguage('terminal', function() {
        return {
          contains: [
            {
              className: 'string',
              begin: '^([\\w.]+)@([\\w.]+)'
            },
            {
              className: 'constant',
              begin: ' (.*) \\$ '
            },
            {
              className: 'ansi',
              begin: '<span style\\="([^"]+)">',
              end: '<\\/span>'
            }
          ]
        }
      });
      var slideshow = remark.create({
        highlightStyle: 'monokai'
      });

      // extract the embedded styling from ansi spans
      var highlighted = document.querySelectorAll("code.terminal span.hljs-ansi");
      Array.prototype.forEach.call(highlighted, function(next) {
        next.insertAdjacentHTML("beforebegin", next.textContent);
        next.parentNode.removeChild(next);
      });

      // Setup MathJax
      MathJax.Hub.Config({
          tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
          }
      });

      MathJax.Hub.Configured();
    </script>
  </body>
</html>