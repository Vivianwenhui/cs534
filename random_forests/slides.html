<!DOCTYPE html>
<html>
  <head>
    <title>RF</title>
    <meta charset="utf-8">
    <link rel="stylesheet" href="https://unpkg.com/purecss@1.0.1/build/pure-min.css" integrity="sha384-oAOxQR6DkCoMliIh8yFnu25d7Eq/PHS21PClpwjOTeU2jRSq11vu66rf90/cZr47" crossorigin="anonymous">    <style>
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
      .reference{
      	font-size: 10px;
      }
      .smaller-font { font-size:14px } 
      @page {
        size: 908px 681px;
        margin: 0;
      }

      @media print {
        .remark-slide-scaler {
          width: 100% !important;
          height: 100% !important;
          transform: scale(1) !important;
          top: 0 !important;
          left: 0 !important;
        }
      }

      .green {
        color: #45ADA8;
      }

      .figure img{
        height: 550px;
      }

      .figure-200 img{
        height: 200px;
      }

      .figure-250 img{
        height: 250px;
      }

      .figure-300 img{
        height: 300px;
      }

      .figure-350 img{
        height: 350px;
      }

      .figure-w500 img{
        width: 500px;
      }

      .figure-w600 img{
        width: 600px;
      }
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

# Random Forests

CS534 - Machine Learning

Yubin Park, PhD

---

class: center, middle

"If you can't beat 'em, join 'em."

To fight with the variance (or randomness), 

we will adopt randomness to the limit.

---

class: middle

## Basic Decision Tree

- Start with a dataset: `\(\mathcal{D} = \{(\mathbf{x}_1, y_1), (\mathbf{x}_2, y_2), \cdots, (\mathbf{x}_n, y_n)\}\)`
- Grow a Decision Tree:
  1. Iterate over all possible splittig pairs: splitting variable and value
  1. Select the best splitting pair
  1. Split the data into two partitions based on the selected splitting pair
  1. Repeat the process till any stopping criterion is met

---

class: middle

## Bagged Trees

- Start with a dataset: `\(\mathcal{D} = \{(\mathbf{x}_1, y_1), (\mathbf{x}_2, y_2), \cdots, (\mathbf{x}_n, y_n)\}\)`
- .green[**Boostrap datasets:**] `\(\mathcal{D}_1, \mathcal{D}_2, \cdots, \mathcal{D}_B\)`
- Grow a Decision Tree .green[**for each bootstrapped dataset:**]
  1. Iterate over all possible splittig pairs: splitting variable and value
  1. Select the best splitting pair
  1. Split the data into two partitions based on the selected splitting pair
  1. Repeat the process till any stopping criterion is met
- .green[**Combine the trained decision trees**]

.reference[[Leo Breiman. Bagging Predictors, Machine Learning (1996)](https://www.stat.berkeley.edu/~breiman/bagging.pdf)]

---

class: middle

## Random Subspace

- Start with a dataset: `\(\mathcal{D} = \{(\mathbf{x}_1, y_1), (\mathbf{x}_2, y_2), \cdots, (\mathbf{x}_n, y_n)\}\)`
- Grow .green[**multiple**] Decision Tree using .green[**the same training data**]
  1. .green[**Bootstrap features at each node**]
  1. Iterate over all possible splittig pairs .green[**within the bootstrapped features**]
  1. Select the best splitting pair
  1. Split the data into two partitions based on the selected splitting pair
  1. Repeat the process till any stopping criterion is met
- .green[**Combine the trained decision trees**]


.reference[[Tin Kam Ho. The Random Subspace Method for
Constructing Decision Forests, IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE (1998)](https://pdfs.semanticscholar.org/b41d/0fa5fdaadd47fc882d3db04277d03fb21832.pdf)]

---

class: middle

## Random Forests

- Start with a dataset: `\(\mathcal{D} = \{(\mathbf{x}_1, y_1), (\mathbf{x}_2, y_2), \cdots, (\mathbf{x}_n, y_n)\}\)`
- .green[**Boostrap datasets:**] `\(\mathcal{D}_1, \mathcal{D}_2, \cdots, \mathcal{D}_B\)`
- Grow a Decision Tree .green[**for each bootstrapped dataset:**]
  1. .green[**Bootstrap features at each node**]
  1. Iterate over all possible splittig pairs .green[**within the bootstrapped features**]
  1. Select the best splitting pair
  1. Split the data into two partitions based on the selected splitting pair
  1. Repeat the process till any stopping criterion is met
- .green[**Combine the trained decision trees**]


.reference[[Leo Breiman. Random Forests, Machine Learning (2001)](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf)]

---

class: middle

## Extra-Trees

- Start with a dataset: `\(\mathcal{D} = \{(\mathbf{x}_1, y_1), (\mathbf{x}_2, y_2), \cdots, (\mathbf{x}_n, y_n)\}\)`
- Grow .green[**multiple**] Decision Tree using .green[**the same training data**]
  1. .green[**Bootstrap features at each node**]
  1. .green[**Draw random split values for the bootstraped features**]
  1. Iterate over all .green[**the candidate splittig pairs**]
  1. Select the best splitting pair
  1. Split the data into two partitions based on the selected splitting pair
  1. Repeat the process till any stopping criterion is met
- .green[**Combine the trained decision trees**]


.reference[[Pierre Geurts, Damien Ernst, Louis Wehenkel. Extremely randomized trees, Machine Learning (2006)](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.65.7485&rep=rep1&type=pdf)]

---

class: center, middle

.figure-w600[![rf1](img/rf1.png)]

.reference[Chapter 15 of [ESLII](https://web.stanford.edu/~hastie/ElemStatLearn/)]

---

class: center, middle

.figure-w600[![rf1](img/rf2.png)]

.reference[Chapter 15 of [ESLII](https://web.stanford.edu/~hastie/ElemStatLearn/)]

---

class: center, middle

.figure-w600[![rf1](img/rf3.png)]

.reference[Chapter 15 of [ESLII](https://web.stanford.edu/~hastie/ElemStatLearn/)]

---

class: center, middle

## Questions?

</textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js">
    </script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML&delayStartupUntil=configured" type="text/javascript">
    </script>
    <script type="text/javascript">
      var hljs = remark.highlighter.engine;
      /*
        Language: terminal console
        Author: Josh Bode <joshbode@gmail.com>
      */
      hljs.registerLanguage('terminal', function() {
        return {
          contains: [
            {
              className: 'string',
              begin: '^([\\w.]+)@([\\w.]+)'
            },
            {
              className: 'constant',
              begin: ' (.*) \\$ '
            },
            {
              className: 'ansi',
              begin: '<span style\\="([^"]+)">',
              end: '<\\/span>'
            }
          ]
        }
      });
      var slideshow = remark.create({
        highlightStyle: 'monokai'
      });

      // extract the embedded styling from ansi spans
      var highlighted = document.querySelectorAll("code.terminal span.hljs-ansi");
      Array.prototype.forEach.call(highlighted, function(next) {
        next.insertAdjacentHTML("beforebegin", next.textContent);
        next.parentNode.removeChild(next);
      });

      // Setup MathJax
      MathJax.Hub.Config({
          tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
          }
      });

      MathJax.Hub.Configured();
    </script>
  </body>
</html>